# -*- coding: utf-8 -*-
"""FINAL-RESNET50-ADSL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qwgOFhbJV5ok7zKYv4FaGnctRrEDlp_c
"""

!pip install tf-explain
!pip install tensorflow-gpu
!pip install tf-nightly
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, BatchNormalization, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import Model, layers
from tensorflow.keras.models import load_model, model_from_json

import tensorflow as tf
import os
import numpy as np
import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
from PIL import Image

#import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras import Model, layers
from tensorflow.keras.models import load_model, model_from_json

tf.keras.__version__  # should be 2.2.2

import PIL
PIL.__version__  # should be 5.2.0

from google.colab import drive
drive.mount('/content/drive')

cd '/content/drive/My Drive/ADSL'

train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
     validation_split=0.2) # set validation split


train_generator = train_datagen.flow_from_directory(
   'train',
    target_size=(64,64),
    batch_size=32,
    color_mode="rgb",
    class_mode='categorical',
    shuffle=True) # set as training data

validation_generator = train_datagen.flow_from_directory(
    'train', 
    target_size=(64,64),
    batch_size=32,
    color_mode="rgb",
    class_mode='categorical',
    shuffle=True,
    subset='validation') # set as validation data

conv_base = ResNet50(include_top=False, weights='imagenet', input_shape=(64,64,3))

conv_base.layers[0]=False
# for layer in conv_base.layers:
#     layer.trainable = False

x = conv_base.output
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(128, activation=tf.nn.relu)(x) 
predictions = layers.Dense(2, activation=tf.nn.softmax)(x)
model = Model(conv_base.input, predictions)
model.summary( )

optimizer = tf.keras.optimizers.Adam(lr=0.00001)
model.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

history = model.fit_generator(generator=train_generator,
                              # steps_per_epoch=347 // 32,  # added in Kaggle
                              epochs=5,
                              validation_data=validation_generator
                              # validation_steps=10  # added in Kaggle
                             )

history.history

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,3.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

print("Training Accuracy : " + str(acc[4]))
print("Training Loss : " + str(loss[4]))
print("Validation Accuracy : " + str(val_acc[4]))
print("Validation Loss : " + str(val_loss[4]))

# !mkdir modelss
 #!mkdir modelss/keras

# save
model.save('modelss/keras-final/model-adsl1-resnet1.h5')

# save
model.save_weights('modelss/keras-final/weights-resnet1.h5')
with open('modelss/keras-final/architecture-resnet1.json', 'w') as f:
        f.write(model.to_json())

# load
#model = load_model('modelss/keras/model-adsl1-resnet1.h5')

# load
#with open('modelss/keras/architecture-resnet1.json') as f:
#    model = model_from_json(f.read())
#model.load_weights('modelss/keras/weights-resnet1.h5')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

from os import listdir
from os.path import isfile, join
from PIL import Image
import PIL
PIL.__version__  # should be 5.2.0

input_path = 'test/test-final/'
validation_img_paths = []
img_list = []

validation_img_paths = [f for f in listdir(input_path) if isfile(join(input_path, f))]
#print(listdir(input_path))
validation_img_paths.sort()
img_list = [Image.open(input_path + img_path) for img_path in validation_img_paths]
print(str(len(img_list))+" files")

class_n =['Artificial','Natural']

total = len(img_list)
suma = 0
sumn = 0
sume = 0

for i, img in enumerate(img_list):
  name = img.filename.split("/")
  img = load_img(img.filename, target_size=(64, 64))  # this is a PIL image
  x = img_to_array(img)  # Numpy array with shape (150, 150, 3)
  x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)
  pred = model.predict(x)

  plt.figure(figsize=(2, 2))
  plt.imshow(img)
 
  plt.title(str(i+1) +". " + name[2]+ " = " + class_n[pred.argmax()])
  plt.show()

  if pred.argmax() == 1:
    sumn = sumn + 1
  else:
    suma = suma + 1

print('============================================================')
print('Total data : ' + str(total) +'('+ str(total/total*100) +'%)')
print('')
print('Artifial Landcover : ' + str(suma)+'('+ str(suma/total*100) +'%)')
print('Natural Landcover : ' + str(sumn)+'('+ str(sumn/total*100) +'%)')
print('')
print('Error : ' + str(sume)+'('+ str(sume/total*100) +'%)')
print('============================================================')

input_path = 'test/slices5/180311/images/'
validation_img_paths = []
img_list = []

validation_img_paths = [f for f in listdir(input_path) if isfile(join(input_path, f))]
#print(listdir(input_path))
validation_img_paths.sort()
img_list = [Image.open(input_path + img_path) for img_path in validation_img_paths]
print(str(len(img_list))+" files")

class_n =['Artificial','Natural']

total = len(img_list)
suma = 0
sumn = 0
sume = 0

for i, img in enumerate(img_list):
  name = img.filename.split("/")
  img = load_img(img.filename, target_size=(64, 64))  # this is a PIL image
  x = img_to_array(img)  # Numpy array with shape (150, 150, 3)
  x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)
  pred = model.predict(x)
 
  print(str(i+1) +". " + name[4]+ " = " + class_n[pred.argmax()])

  if pred.argmax() == 1:
    sumn = sumn + 1
  else:
    suma = suma + 1

print('============================================================')
print('Total data : ' + str(total) +'('+ str(total/total*100) +'%)')
print('')
print('Artifial Landcover : ' + str(suma)+'('+ str(suma/total*100) +'%)')
print('Natural Landcover : ' + str(sumn)+'('+ str(sumn/total*100) +'%)')
print('')
print('Error : ' + str(sume)+'('+ str(sume/total*100) +'%)')
print('============================================================')

input_path = 'test/slices5/181002/images/'
validation_img_paths = []
img_list = []

validation_img_paths = [f for f in listdir(input_path) if isfile(join(input_path, f))]
#print(listdir(input_path))
validation_img_paths.sort()
img_list = [Image.open(input_path + img_path) for img_path in validation_img_paths]
print(str(len(img_list))+" files")

class_n =['Artificial','Natural']

total = len(img_list)
suma = 0
sumn = 0
sume = 0

for i, img in enumerate(img_list):
  name = img.filename.split("/")
  img = load_img(img.filename, target_size=(64, 64))  # this is a PIL image
  x = img_to_array(img)  # Numpy array with shape (150, 150, 3)
  x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)
  pred = model.predict(x)
 
  print(str(i+1) +". " + name[4]+ " = " + class_n[pred.argmax()])

  if pred.argmax() == 1:
    sumn = sumn + 1
  else:
    suma = suma + 1

print('============================================================')
print('Total data : ' + str(total) +'('+ str(total/total*100) +'%)')
print('')
print('Artifial Landcover : ' + str(suma)+'('+ str(suma/total*100) +'%)')
print('Natural Landcover : ' + str(sumn)+'('+ str(sumn/total*100) +'%)')
print('')
print('Error : ' + str(sume)+'('+ str(sume/total*100) +'%)')
print('============================================================')

input_path = 'test/slices5/190528/images/'
validation_img_paths = []
img_list = []

validation_img_paths = [f for f in listdir(input_path) if isfile(join(input_path, f))]
#print(listdir(input_path))
validation_img_paths.sort()
img_list = [Image.open(input_path + img_path) for img_path in validation_img_paths]
print(str(len(img_list))+" files")

class_n =['Artificial','Natural']

total = len(img_list)
suma = 0
sumn = 0
sume = 0

for i, img in enumerate(img_list):
  name = img.filename.split("/")
  img = load_img(img.filename, target_size=(64, 64))  # this is a PIL image
  x = img_to_array(img)  # Numpy array with shape (150, 150, 3)
  x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)
  pred = model.predict(x)
 
  print(str(i+1) +". " + name[4]+ " = " + class_n[pred.argmax()])

  if pred.argmax() == 1:
    sumn = sumn + 1
  else:
    suma = suma + 1

print('============================================================')
print('Total data : ' + str(total) +'('+ str(total/total*100) +'%)')
print('')
print('Artifial Landcover : ' + str(suma)+'('+ str(suma/total*100) +'%)')
print('Natural Landcover : ' + str(sumn)+'('+ str(sumn/total*100) +'%)')
print('')
print('Error : ' + str(sume)+'('+ str(sume/total*100) +'%)')
print('============================================================')

input_path = 'test/slices5/190622/images/'
validation_img_paths = []
img_list = []

validation_img_paths = [f for f in listdir(input_path) if isfile(join(input_path, f))]
#print(listdir(input_path))
validation_img_paths.sort()
img_list = [Image.open(input_path + img_path) for img_path in validation_img_paths]
print(str(len(img_list))+" files")

class_n =['Artificial','Natural']

total = len(img_list)
suma = 0
sumn = 0
sume = 0

for i, img in enumerate(img_list):
  name = img.filename.split("/")
  img = load_img(img.filename, target_size=(64, 64))  # this is a PIL image
  x = img_to_array(img)  # Numpy array with shape (150, 150, 3)
  x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)
  pred = model.predict(x)
 
  print(str(i+1) +". " + name[4]+ " = " + class_n[pred.argmax()])

  if pred.argmax() == 1:
    sumn = sumn + 1
  else:
    suma = suma + 1

print('============================================================')
print('Total data : ' + str(total) +'('+ str(total/total*100) +'%)')
print('')
print('Artifial Landcover : ' + str(suma)+'('+ str(suma/total*100) +'%)')
print('Natural Landcover : ' + str(sumn)+'('+ str(sumn/total*100) +'%)')
print('')
print('Error : ' + str(sume)+'('+ str(sume/total*100) +'%)')
print('============================================================')

input_path = 'test/slices5/191106/images/'
validation_img_paths = []
img_list = []

validation_img_paths = [f for f in listdir(input_path) if isfile(join(input_path, f))]
#print(listdir(input_path))
validation_img_paths.sort()
img_list = [Image.open(input_path + img_path) for img_path in validation_img_paths]
print(str(len(img_list))+" files")

class_n =['Artificial','Natural']

total = len(img_list)
suma = 0
sumn = 0
sume = 0

for i, img in enumerate(img_list):
  name = img.filename.split("/")
  img = load_img(img.filename, target_size=(64, 64))  # this is a PIL image
  x = img_to_array(img)  # Numpy array with shape (150, 150, 3)
  x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)
  pred = model.predict(x)
 
  print(str(i+1) +". " + name[4]+ " = " + class_n[pred.argmax()])

  if pred.argmax() == 1:
    sumn = sumn + 1
  else:
    suma = suma + 1

print('============================================================')
print('Total data : ' + str(total) +'('+ str(total/total*100) +'%)')
print('')
print('Artifial Landcover : ' + str(suma)+'('+ str(suma/total*100) +'%)')
print('Natural Landcover : ' + str(sumn)+'('+ str(sumn/total*100) +'%)')
print('')
print('Error : ' + str(sume)+'('+ str(sume/total*100) +'%)')
print('============================================================')